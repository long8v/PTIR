[![Hits](https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fgithub.com%2Flong8v%2FPTIR&count_bg=%2379C83D&title_bg=%23555555&icon=&icon_color=%23E7E7E7&title=hits&edge_flat=false)](https://hits.seeyoufarm.com) 
# ğŸ“š Paper Today I Read ğŸ““
- 21ë…„ë„ê¹Œì§€ `readme.md` + `notion`, 22ë…„ë¶€í„° [issue](https://github.com/long8v/PTIR/issues)ë¡œ ê´€ë¦¬
- 22ë…„ë„ ëª©í‘œëŠ” 100ê°œ! í˜„ì¬  <img alt="issues" src="https://img.shields.io/github/issues/long8v/PTIR?color=0088ff"> ê°œ

## 2021ë…„ì— ì½ì€ ë…¼ë¬¸ë“¤ (28 papers)
  * [2021-12-31 DeiT](#2021-12-31-deit)
  * [2021-12-30 LeViT](#2021-12-30-levit)
  * [2021-12-28 Frozen](#2021-12-28-frozen)
  * [2021-12-27 DETR](#2021-12-27-detr)
  * [2021-12-23 Pix2seq](#2021-12-23-pix2seq)
  * [2021-12-21 Swin](#2021-12-21-swin)
  * [2021-12-20 SimpleDLM](#2021-12-20-simpledlm)
  * [2021-12-17 METER](#2021-12-17-meter)
  * [2021-12-16 Gated MoE](#2021-12-16-gated-moe)
  * [2021-12-15 GLaM](#2021-12-15-glam)
  * [2021-12-14 PVT](#2021-12-14-pvt)
  * [2021-12-13 MixText](#2021-12-13-mixtext)
  * [2021-12-09 Linformer](#2021-12-09-linformer)
  * [2021-12-07 T5](#2021-12-07-t5)
  * [2021-12-06 StructuralLM](#2021-12-06-structurallm)
  * [2021-12-03 BROS](#2021-12-03-bros)
  * [2021-12-02 Donut](#2021-12-02-donut)
  * [2021-12-01](#2021-12-01)
  * [2021-11-30 Copying Network](#2021-11-30-copying-network)
  * [2021-11-29 SPADE](#2021-11-29-spade)
  * [2021-11-26 SQLova](#2021-11-26-sqlova)
  * [2021-11-25 SPADE](#2021-11-25-spade)
  * [2021-09-05 TSDAE](#2021-09-05-tsdae)
  * [2021-09-04 Faster R-CNN](#2021-09-04-faster-r-cnn)
  * [2021-09-03 ViT](#2021-09-03-vit)
  * [2021-09-01 LayoutLM](#2021-09-01-layoutlm)
  * [2021-08-30 BART](#2021-08-30-bart)
  * [2021-08-20](#2021-08-20)

## 2021-12-31 DeiT
Training data-efficient image transformers & distillation through attention, 2021([arxiv](https://arxiv.org/abs/2012.12877))<br>
**problem :** ViTëŠ” ëŒ€ê·œëª¨ ì´ë¯¸ì§€ ë°ì´í„°ì…‹ìœ¼ë¡œ í•™ìŠµí•´ì•¼ ì¼ë°˜í™” ê°€ëŠ¥í•œ ëª¨ë¸ì„ í•™ìŠµí•  ìˆ˜ ìˆìŒ<br>
**solution :** ViTêµ¬ì¡°ì— distillation tokenì„ ì¶”ê°€í•˜ì—¬ CNN ë“±ì˜ teacher modelì„ ì‚¬ìš©í•˜ì—¬ distillation í•™ìŠµì„ ì§„í–‰í•¨<br>
**result :** imageNetë§Œìœ¼ë¡œ í•™ìŠµí•œ ëª¨ë¸ì´ ì¢‹ì€ ì •í™•ë„ë¥¼ ëƒ„, ViT, ResNetì™€ ë¹„êµí•´ë´¤ì„ ë•Œ ë” ì‘ì€ íŒŒë¼ë¯¸í„°ë¡œ ë‚˜ì€ ì„±ëŠ¥<br>
**details :** [notion](https://long8v.notion.site/DeiT-a045232cb9f4468e9b90e6a3efda8625)

## 2021-12-30 LeViT
LeViT: a Vision Transformer in ConvNet's Clothing for Faster Inference, 2021([arxiv](https://arxiv.org/pdf/2104.01136.pdf))<br>
**problem :** ë” ë‚˜ì€ ì†ë„/ì„±ëŠ¥ trade-offë¥¼ ê°€ì§€ëŠ” ViTë¥¼ ê°œë°œí•´ë³´ì<br>
**solution :** ì²˜ìŒì— CNNìœ¼ë¡œ resolutionì„ ì¤„ì¸ ë’¤, íŠ¸ëœìŠ¤í¬ë¨¸ë¥¼ ìœ„ì— ë¶™ì„. ì´ë•Œ, ìš°ë¦¬ëŠ” CNNì²˜ëŸ¼ pyramid êµ¬ì¡°ë¥¼ ê°–ê¸° ìœ„í•´ ì¤‘ê°„ì— Queryë¥¼ average-poolingìœ¼ë¡œ sub-samplingì„ í•˜ì—¬ resolutionì„ ì¤‘ê°„ì¤‘ê°„ì— ì¤„ì—¬ê°. ì™¸ì— MLP ëŒ€ì‹  1 x 1 convolutionì„ í•˜ê³ , positional embedding ëŒ€ì‹  attention biasë¥¼ ì¶”ê°€í•˜ëŠ” ë“± íš¨ìœ¨ì ì¸ ì—°ì‚°ì„ ìœ„í•œ ë””í…Œì¼ì„ ìˆ˜ì •í•¨.<br>
**result :** DeiTë‚˜ EfficientNetê³¼ ìœ ì‚¬í•œ ì„±ëŠ¥ìœ¼ë¡œ ë” ë¹ ë¥¸ ì¸í¼ëŸ°ìŠ¤ ì†ë„.<br>
**details :** [notion](https://long8v.notion.site/LeViT-8dfba651c7e54430992ee79b2e3429c6)<br>

## 2021-12-28 Frozen
Multimodal Few-Shot Learning with Frozen Language Models, 2021([arxiv](https://papers.nips.cc/paper/2021/file/01b7575c38dac42f3cfb7d500438b875-Paper.pdf))<br>
**problem :** ëŒ€í˜• ì–¸ì–´ëª¨ë¸ì—ê²Œ visual ì •ë³´ë¥¼ few-shotìœ¼ë¡œ í•™ìŠµí•  ìˆ˜ ìˆê²Œ í•´ë³´ì<br>
**solution :** visual encoderë¥¼ prefixì²˜ëŸ¼ input ì‹œí€€ìŠ¤ ì•ì— ë‘ê³  ê¸°ì¡´ ì–¸ì–´ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„°ëŠ” frozen ì‹œí‚¤ê³  í•™ìŠµì‹œí‚´. <br>
**result :** few-shotìœ¼ë¡œë„ ìƒë‹¹í•œ ì„±ëŠ¥, ë©€í‹°ëª¨ë‹¬ few shot ëŸ¬ë‹ ë°´ì¹˜ë§ˆí¬ë¥¼ ì œì•ˆ<br>
**details :** [notion](https://long8v.notion.site/Frozen-405d8913a0ea4779a503e8f61e21d835)<br>

## 2021-12-27 DETR
End-to-End Object Detection with Transformers, 2020([arxiv](https://arxiv.org/pdf/2005.12872.pdf))<br>
**problem :** object detection ë¬¸ì œë¥¼ í’€ê¸° ìœ„í•´ì„  ìˆ˜ì‘ì—… êµ¬ì¡°/ì„¤ê³„ê°€ í•„ìš”í•¨<br>
**solution :** object detectionì´ ì¤‘ë³µì´ ì—†ëŠ” ìˆœì„œ ìƒê´€ ì—†ëŠ” setì„ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì´ê¸° ë•Œë¬¸ì—, CNN + transformer encoder-decoder + FFNìœ¼ë¡œ í•œë²ˆì— bboxë¥¼ ì˜ˆì¸¡í•  ìˆ˜ ìˆë„ë¡ í•¨. ì´ ë•Œ, lossëŠ” ìš°ì„  ì˜ˆì¸¡ëœ boxì™€ gt boxë¥¼ bipartiteë¡œ ìµœì  matchingì„ êµ¬í•œ ë’¤, ìµœì  ë§¤ì¹­ì—ì„œ box lossì™€ í´ë˜ìŠ¤ ë ˆì´ë¸”ì„ í•©ì¹œ lossë¥¼ ì‚¬ìš©í•¨.<br>
**result :** FASTER RCNNê³¼ ìœ ì‚¬í•œ ì„±ëŠ¥, panoptic segmentationì—ì„œ SOTA<br>
**details :** [notion](https://long8v.notion.site/DETR-5810bf27ec954498a3bdd95c15b116b7)<br>

## 2021-12-23 Pix2seq
Pix2seq: A Language Modeling Framework for Object Detection, 2021([arxiv](https://arxiv.org/pdf/2109.10852.pdf))<br>
**problem :** Object Detection ë¬¸ì œë¥¼ í’€ê¸° ìœ„í•´ì„  íŠ¹ìˆ˜í•œ êµ¬ì¡°/ì„¤ê³„ê°€ í•„ìš”í•¨<br>
**solution :** object detection ë¬¸ì œë¥¼ ì´ë¯¸ì§€ë¥¼ ë„£ì—ˆì„ ë•Œ ë°”ìš´ë”© ë°•ìŠ¤ì™€ ë ˆì´ë¸”ì„ í‘œí˜„í•œ í† í° sequenceë¥¼ ë½‘ëŠ” ì¸ì½”ë”-ë””ì½”ë” êµ¬ì¡°ë¡œ ë°”ê¿ˆ. ì´ë•Œ ëª¨ë“  objectë¥¼ ì°¾ì§€ ì•Šê³  ëë‚˜ë²„ë¦¬ëŠ” ê²ƒì„ ë§‰ê¸° ìœ„í•´ noiseë¥¼ ì„ì€ augmentationì„ ì¶”ê°€í•˜ì—¬ ëª¨ë¸ì´ noiseì¸ì§€ ì•„ë‹Œì§€ë¥¼ êµ¬ë¶„í•˜ë„ë¡ í•˜ë©´ì„œ ê³ ì •ëœ ê¸¸ì´ë¡œ ì˜ˆì¸¡í•˜ë„ë¡ í•¨. ì´ë¥¼ í†µí•´ recallì„ ëŒì–´ì˜¬ë¦¼. <br>
**result :** Faster RCNN, DETRê³¼ ê°™ì€ ë””í…ì…˜ë§Œì„ ìœ„í•´ ì„¤ê³„ë˜ì—ˆê³  ìµœì í™”ëœ ëª¨ë¸ë“¤ê³¼ ì„±ëŠ¥ì´ ìœ ì‚¬í•˜ê²Œ ë‚¨<br>
**details :** [notion](https://long8v.notion.site/pix2seq-109e93c7ebb54104bbca96f16ddc4127)<br>

## 2021-12-21 Swin
Swin Transformer: Hiearchical Vision Transformer using Shifted Window, 2021([arxiv](https://arxiv.org/abs/2103.14030))<br>
**problem :** ViTì™€ ê°™ì´ ë¹„ì „ì— íŠ¸ëœìŠ¤í¬ë¨¸ë¥¼ ì ìš©í•˜ê³ ì í•˜ëŠ” ì‹œë„ê°€ ìˆìœ¼ë‚˜, í•œê°œì˜ í† í° ë‹¨ìœ„ë¥¼ 16 by 16ë¡œ ê³ ì •í•˜ëŠ” ê²ƒì€ pixel ë‹¨ìœ„ì¸ semantic segmentationì„ í•˜ê¸°ì—” ì í•©í•˜ì§€ ì•Šìœ¼ë©° ê³ í™”ì§ˆ ë°ì´í„°ì˜ ê²½ìš° ì´ë¯¸ì§€ í¬ê¸°ì— ì œê³±ìœ¼ë¡œ ì—°ì‚°ì˜ ì–‘ì´ ë§ì•„ì ¸ì„œ ì‚¬ìš©ì— í•œê³„ê°€ ìˆìŒ <br>
**solution :** ViTì²˜ëŸ¼ m by m íŒ¨ì¹˜ë¡œ ìë¥¸ ë’¤ì— ê·¸ ë‚´ë¶€ì˜ í”½ì…€ ë‹¨ìœ„ë¡œ self-attentionì„ í•¨. ê·¸ ë’¤ì— íŒ¨ì¹˜ë¥¼ M // 2ë§Œí¼ shift í•´ì„œ íŒ¨ì¹˜ë¥¼ ë‚˜ëˆˆ ë’¤ self-attentionì„ í•¨. ì´ êµ¬ì¡°ë¥¼ ë°˜ë³µí•˜ì—¬ í”„ë¦¬íŠ¸ë ˆì´ë‹/íŒŒì¸íŠœë‹ì„ ì§„í–‰í•¨ <br>
**result :** ViTì™€ ë‹¬ë¦¬ ì—°ì‚°ëŸ‰ì´ ì´ë¯¸ì§€ í¬ê¸°ì˜ ì„ í˜•ì ìœ¼ë¡œ ì¦ê°€í•˜ë©´ì„œë„ classification, detection, segmentationì—ì„œ SOTA <br>
**details :** [notion](https://long8v.notion.site/Swin-Transformer-99285aee1ff14e3ab5411c3427c50311)<br>

## 2021-12-20 SimpleDLM
Value Retrieval with Arbitrary Queries for Form-like Documents, 2021([arxiv](https://arxiv.org/abs/2112.07820))<br>
**problem :** ë¬¸ì„œì—ì„œ ì›í•˜ëŠ” ì •ë³´ë¥¼ ë½‘ëŠ” íƒœìŠ¤í¬ì—ì„œ ì´ì „ ë°©ë²•ë¡ ë“¤ì€ ë¯¸ë¦¬ ì •ì˜í•´ë†“ì€ fieldë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë¬¸ì œë¥¼ í’€ì—ˆëŠ”ë°, ì´ëŠ” ë‹¤ë¥¸ form í˜¹ì€ ë„ë©”ì¸ì— ì ìš©í•˜ê¸° ì–´ë µë‹¤<br>
**solution :** ì¿¼ë¦¬ê°€ ì£¼ì–´ì¡Œì„ ë•Œ ë¬¸ì„œì—ì„œ ì›í•˜ëŠ” valueë¥¼ ì°¾ëŠ” ë¬¸ì œë¡œ ë°”ê¿ˆ. ì¿¼ë¦¬ì™€ OCR í…ìŠ¤íŠ¸ë¥¼ ê°™ì€ ì„ë² ë”©ì„ ê±°ì¹œ ë’¤ ê°™ì´ self-attentionì„ í•˜ë„ë¡ í•˜ê³  ê°ê°ì˜ ë§ˆì§€ë§‰ ì…€í”„ì–´í…ì…˜ ë ˆì´ì–´ì— average-pooling, FCNì„ ê±°ì¹œ ë’¤ ë‚´ì ì„ í•˜ì—¬ ê° í† í°ì´ ì›í•˜ëŠ” valueì¸ì§€ ì•„ë‹Œì§€ ì´ì§„ ë¶„ë¥˜í•˜ëŠ” ë¬¸ì œë¡œ ë°”ê¿ˆ. í•´ë‹¹ ì•„í‚¤í…ì³ë¡œ MLMì„ í•˜ëŠ” í”„ë¦¬íŠ¸ë ˆì´ë‹ ëª¨ë¸(simpleDLM)ì„ ë§Œë“¦.<br>
**result :** BERT, LayoutLM í”„ë¦¬íŠ¸ë ˆì´ë‹ì„ ê°€ì ¸ì˜¨ ê²ƒë³´ë‹¤ simpleDLMìœ¼ë¡œ í”„ë¦¬íŠ¸ë ˆì´ë‹í•œ ê²ƒì´ F1 ì„±ëŠ¥ì´ ìš°ìœ„. <br>
**details :** [notion](https://long8v.notion.site/simpleDLM-7c1edd8680584952a5c4a2dd2794cbfb)<br>

## 2021-12-17 METER
An Empirical Study of Training End-to-End Vision-and-Language Transformers, 2021([arxiv](https://arxiv.org/pdf/2111.02387.pdf))<br>
**problem :** Vison-and-Language(VL) íƒœìŠ¤í¬ì— ëŒ€í•´ì„œ end-to-end ë°©ì‹ìœ¼ë¡œ íŠ¸ëœìŠ¤í¬ë¨¸ë§Œ ì‚¬ìš©í•œ ëª¨ë¸ ì¤‘ region-based modelë³´ë‹¤ ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì¸ ê²ƒì´ ì—†ì—ˆê³ , ê° ëª¨ë“ˆë“¤ì— ëŒ€í•œ ì‹¤í—˜ì´ ë¶€ì¡±í•¨<br>
**solution :** VLì˜ ì£¼ìš” ëª¨ë“ˆë“¤ì— ëŒ€í•´ ì•Œì•„ë³´ê³  Ablation studyí•¨ <br>
**result :** VQA íƒœìŠ¤í¬ì—ì„œ SOTA, ì‹¤í—˜ì„ í†µí•´ ëª‡ê°€ì§€ ëª¨ë“ˆë“¤ì— ëŒ€í•œ ì„±ëŠ¥ ìš°ìœ„ë¥¼ ì•Œì•„ëƒ„<br>
**details :** [notion](https://long8v.notion.site/METER-b9b1beb61f194ea5ba6d573b16328ead)<br>

## 2021-12-16 Gated MoE
Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer, 2017([arxiv](https://arxiv.org/pdf/1701.06538.pdf))<br>
**problem :** í° ëª¨ë¸ì„ ë§Œë“¤ë©´ ì„±ëŠ¥ì´ ëŠ˜ì–´ë‚˜ì§€ë§Œ ë©”ëª¨ë¦¬ì˜ ì´ìŠˆë¡œ í•œê³„ê°€ ìˆìŒ<br>
**solution :** ê° ë…ë¦½ì ì¸ NNì¸ expertë“¤ì„ ë§Œë“¤ê³ , ì¸í’‹ì— ë”°ë¼ ì–´ë–¤ expertë¥¼ ì„ íƒí• ì§€ gating ë„¤íŠ¸ì›Œí¬ë¥¼ êµ¬ì„±. ì´ë•Œ top kê°œë§Œ ì„ íƒí•˜ë„ë¡ í•˜ì—¬ sparseí•œ gatingì„ ê° expertì˜ outputrê³¼ weighted sumí•˜ì—¬ <br>
**result :** ë” ë‚®ì€ ë¹„ìš©ìœ¼ë¡œ ë” í° ëª¨ë¸ í•™ìŠµ. MTì—ì„œ SOTA<br>
**details :** [notion](https://long8v.notion.site/gated-MoE-ff42470ad545417795e82ea54fefbf3b)<br>

## 2021-12-15 GLaM
GLaM: Efficient Scaling of Language Models with Mixture-of-Experts([arxiv](https://arxiv.org/pdf/2112.06905.pdf))<br>
**problem :** NLPì—ì„œ í° í”„ë¦¬íŠ¸ë ˆì´ë‹ ëª¨ë¸ì€ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì´ì§€ë§Œ, í•™ìŠµ/ì¶”ë¡  ë¹„ìš©ì´ ë„ˆë¬´ í¬ë‹¤<br>
**solution :** Mixture-of-Experts ëª¨ë¸ì—ì„œ ë”°ì™€ íŠ¸ëœìŠ¤í¬ë¨¸ êµ¬ì¡° ë‚´ì— ì£¼ì–´ì§„ í† í°ì„ ì²˜ë¦¬í•˜ê¸°ì— ì–´ë–¤ expertê°€ ê°€ì¥ ì í•©í•œì§€ë¥¼ í•™ìŠµí•˜ëŠ” gatingì„ ë§Œë“¤ê³  ì„ ì •ëœ expertì˜ outputì˜ í•©ìœ¼ë¡œ ëª¨ë¸ì˜ outputì„ ë‚´ë±‰ìŒ<br>
**result :** GPT-3ë³´ë‹¤ í¬ê¸°ê°€ 7ë°° í¬ì§€ë§Œ, ì—ë„ˆì§€ëŠ” 1/3ë°° ì“°ì´ë©° zero-shot, one-shotì—ì„œ GPT-3ë³´ë‹¤ ì„±ëŠ¥ ìš°ìœ„<br>
**details :** [notion](https://long8v.notion.site/GLaM-051d25d6164b4d4bb4f6191beeeba81b)<br>

## 2021-12-14 PVT
Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions([arxiv](https://arxiv.org/pdf/2102.12122.pdf))<br>
**problem :** ë¹„ì „ íŠ¸ëœìŠ¤í¬ë¨¸ ë¶„ì•¼ì—ì„œ ì´ì „ì— ì œì‹œëœ ViTëŠ” ì—°ì‚°ëŸ‰ì´ ë§ê³  ì•„ì›ƒí’‹ì´ ì €ì°¨ì›ì´ì–´ì„œ í”½ì…€ ë ˆë²¨ì˜ íƒœìŠ¤í¬ë¥¼ í•˜ê¸°ì— ì ì ˆí•˜ì§€ ì•ŠìŒ<br>
**solution :** 1) Feature Pyramid : CNNì²˜ëŸ¼ 4ë‹¨ê³„ì˜ ë ˆì´ì–´ë¥¼ ìŒ“ì•„ì„œ ê° í¬ê¸°ì˜ í”¼ì³ë¥¼ ëª¨ë‘ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ í•¨. 2) Spatial Reduction Attention(SRA) : Self-Attention ì—°ì‚°ì—ì„œ Keyì™€ Valueë¥¼ reshape + FCë¥¼ ê±°ì³ ì°¨ì›ì„ ì¤„ì¸ ë’¤ì— ì§„í–‰í•¨<br>
**result :** ì´ë¯¸ì§€ ë¶„ë¥˜ ë¿ ì•„ë‹ˆë¼ ë””í…ì…˜/ì„¸ê·¸ë©˜í…Œì´ì…˜ ê°™ì€ dense prediction íƒœìŠ¤í¬ì—ë„ ë°”ë¡œ ì ìš© ê°€ëŠ¥í•˜ë©°, SRAë¥¼ í†µí•´ ê³„ì‹¼ ë¹„ìš©ë„ ì¤„ì„. ë‹¤ì–‘í•œ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ íƒœìŠ¤í¬ì—ì„œ SOTA<br>
**details :** [notion](https://long8v.notion.site/PVT-6403ee04d45c4732a02f65c7924f1f08)<br>

## 2021-12-13 MixText
MixText: Linguistically-Informed Interpolation of Hidden Space for Semi-Supervised Text Classification([arxiv](https://aclanthology.org/2020.acl-main.194.pdf))<br>
**problem :** ê¸°ì¡´ì˜ semi-supervised ë°©ë²•ë¡ ë“¤ì€ label ë°ì´í„°ì™€ unlabeled ë°ì´í„°ê°€ ë”°ë¡œ ì‚¬ìš©ë˜ì–´, unlabeled ë°ì´í„°ì—ì„œ íŒŒì¸íŠœë‹ ì‹œ ì—¬ì „íˆ ê³¼ì í•©ë  ê°€ëŠ¥ì„±ì´ ë§ìŒ<br>
**solution :** ë¹„ì „ì—ì„œ ì‚¬ìš©ë˜ëŠ” mixup ê¸°ë²•ê³¼ ìœ ì‚¬í•˜ê²Œ, BERT ë“±ì˜ í”„ë¦¬íŠ¸ë ˆì¸ ëª¨ë¸ì— ì„ì˜ì˜ ë‘ ê°œì˜ xì˜ hidden vectorë¥¼ ë³´ê°„í•˜ê³  yì—­ì‹œ ë³´ê°„í•˜ì—¬ í•™ìŠµë°ì´í„°ë¡œ ë ˆì´ë¸” ë°ì´í„°ì™€ í•¨ê»˜ ì‚¬ìš©. KL divergence lossì— unlabeled dataì— ëŒ€í•´ ë” ìì‹ ìˆê²Œ ì˜ˆì¸¡í•  ìˆ˜ ìˆë„ë¡ ë¶„ë¥˜ ì—”íŠ¸ë¡œí”¼ ë¡œìŠ¤ë¥¼ í•©í•˜ì—¬ lossë¡œ ì‚¬ìš©<br>
**result :** í…ìŠ¤íŠ¸ ë¶„ë¥˜ ë¬¸ì œì—ì„œ ì œí•œëœ ë ˆì´ë¸” ë°ì´í„°ë¥¼ ê°€ì§€ê³  ìˆì„ ë•Œ SOTA<br>
**details :** [notion](https://long8v.notion.site/MixText-c91fca74f6bb46fe9ef0aa8868fc5bd4)<br>

## 2021-12-09 Linformer
Linformer: Self-Attention with Linear Complexity([arxiv](https://arxiv.org/abs/2006.04768))<br>
**problem :** Transformerì˜ self-attention ì—°ì‚°ì´ ì‹œí€€ìŠ¤ ê¸¸ì´ nì— ëŒ€í•˜ì—¬ O(n^2)ë¡œ ì‹œê°„, ê³µê°„ë³µì¡ë„ê°€ ëŠ˜ì–´ë‚¨. <br>
**solution :** self-attention layerì˜ ê²°ê³¼ matrixê°€ low-rankë¼ëŠ” ê²ƒì„ ë°í˜€ëƒ„. ì¦‰, ì €ì°¨ì›ìœ¼ë¡œ í‘œí˜„í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ linear projection ë ˆì´ì–´ë¥¼ ì¶”ê°€í•˜ì—¬ key, valueì˜ ì°¨ì›ì„ ì¤„ì¸ ë’¤ ì—°ì‚°ì„ ì§„í–‰í•¨.<br>
**result :** RoBERTaì™€ ì„±ëŠ¥ì€ ìœ ì‚¬í•˜ê±°ë‚˜ ë” ë‚«ì§€ë§Œ, ì‹œê°„/ê³µê°„ë³µì¡ë„ë¥¼ O(n)ìœ¼ë¡œ ì¤„ì„. <br>
**details :** [notion](https://long8v.notion.site/LinFormer-7aec3be7c2d349b6aa227806955dba61)<br>

## 2021-12-07 T5
Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer([arxiv](https://arxiv.org/pdf/1910.10683.pdf))<br>
**problem :** ë‹¤ì–‘í•œ NLP í”„ë¦¬íŠ¸ë ˆì´ë‹ ê¸°ë²•ì€ ë°ì´í„°/ëª©í‘œí•¨ìˆ˜/êµ¬ì¡° ë“±ì´ ë‹¬ë¼ ì„œë¡œì˜ ì„±ëŠ¥ í˜¹ì€ íš¨ê³¼ì„±ì„ ë¹„êµí•˜ê¸° ì–´ë ¤ì›€<br>
**solution :** encoder-decoder êµ¬ì¡°ë¡œ í”„ë¦¬íŠ¸ë ˆì´ë‹ ì‹œì—ëŠ” ì„ì˜ë¡œ ì„ íƒëœ ì—°ì†ëœ í† í° ê°ì¶°ë†“ê³  ì´ë¥¼ ë§ì¶”ëŠ” ë¬¸ì œë¥¼ í’ˆ. íŒŒì¸íŠœë‹ ì‹œì—ëŠ” ì¸í’‹ ì•ì— finetuning íƒœìŠ¤í¬ë¥¼ ë‚˜íƒ€ë‚´ëŠ” prefixë¥¼ ë¶™ì´ê³  decoderëŠ” ê° íƒœìŠ¤í¬ë³„ outputì„ ìƒì„±í•˜ë„ë¡ í•¨<br>
**result :** ë‹¤ì–‘í•œ íƒœìŠ¤í¬ì—ì„œ SOTA, í•˜ë‚˜ì˜ text-to-text í”„ë ˆì„ì›Œí¬ë¡œ í”„ë¦¬íŠ¸ë ˆì´ë‹/íŒŒì¸íŠœë‹ì„ ì§„í–‰í•˜ì—¬ ëª¨ë¸ ë° ë°ì´í„°ì˜ ë³€ë™ì— ë”°ë¥¸ ì„±ëŠ¥ ì°¨ì´ë¥¼ ë¶„ì„í•  ìˆ˜ ìˆê²Œ í•¨<br>
**details :** [notion](https://long8v.notion.site/T5-9beb63d63a5c4a89acd524056d1bbe60)<br>

## 2021-12-06 StructuralLM
StructuralLM: Structural Pre-training for Form Understanding([arxiv](https://arxiv.org/abs/2105.11210))<br>
**problem :** ë¬¸ì„œë¥¼ ì´í•´í•˜ëŠ” pretraining ëª¨ë¸ì˜ ê²½ìš° ë¬¸ì„œì˜ ì…€(OCRì˜ ë°”ìš´ë”©ë°•ìŠ¤)ì˜ semanticí•œ ì •ë³´ë¥¼ ì¶©ë¶„íˆ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ. (ì…€ ë‚´ì˜ í† í°ë“¤ì´ ê°™ì€  <br>
**solution :** 2D poisitonal embddingì„ í•˜ê³  ì…€ ë‚´ì˜ í† í°ì€ ìˆœì„œì— ë”°ë¥¸ 1D positional embeddingì„ í•¨. pre-training taskì— ìˆ¨ê²¨ì§„ ì…€ì´ ë¬¸ì„œì˜ ì–´ëŠ ì˜ì—­ì— ìœ„ì¹˜í•˜ëŠ”ì§€ ë¶„ë¥˜í•˜ëŠ” Cell Poisition Classificationì„ ì¶”ê°€í•¨. <br>
**result :** ì´ë¯¸ì§€ ì •ë³´ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šì•˜ìŒì—ë„ SOTA<br>
**details :** [notion](https://long8v.notion.site/structuralLM-64f16e02a47f4e2697b4f488dccf0db1)<br>

## 2021-12-03 BROS
BROS: A Pre-trained Language Model Focusing on Text and Layout for Better Key Information Extraction from Documentsr([arxiv](https://arxiv.org/pdf/2108.04539.pdf))<br>
**problem :** ë¬¸ì„œì—ì„œ Key Informationë¥¼ ë½‘ëŠ” íƒœìŠ¤í¬ì—ì„œ ìµœê·¼ í…ìŠ¤íŠ¸ + ë ˆì´ì•„ì›ƒì— ì´ë¯¸ì§€ ì •ë³´ê¹Œì§€ ì‚¬ìš©í•˜ëŠ”ë° ì´ëŠ” ì—°ì‚°ì ìœ¼ë¡œ ë¹„ìš©ì´ ë§ì´ ë“ ë‹¤<br>
**solution :** LayoutLM + 2D encodingì„ ì ˆëŒ€ì¢Œí‘œê°€ ì•„ë‹Œ í…ìŠ¤íŠ¸ ë°•ìŠ¤ ê°„ì˜ 4 ê¼­ì§€ì ì˜ ê±°ë¦¬ë¥¼ siní•¨ìˆ˜ë¡œ ì„ë² ë”© + spanBERTì™€ ìœ ì‚¬í•˜ê²Œ 2Dì—ì„œ ê·¼ì²˜ í…ìŠ¤íŠ¸ ë°•ìŠ¤ë¥¼ maskí•˜ê³  ë§ì¶”ë„ë¡ í•˜ëŠ” Area-MLM í”„ë¦¬íŠ¸ë ˆì´ë‹ íƒœìŠ¤í¬ë¥¼ ì¶”ê°€í•¨. decodingì€ SPADEë¥¼ í†µí•´ í•¨ <br>
**result :** entity linking, entity extraction íƒœìŠ¤í¬ì—ì„œ SOTA. ë°ì´í„°ì…‹ì´ ì ì„ ê²½ìš°ì—ë„ ì„±ëŠ¥ ì €í•˜ê°€ ë‹¤ë¥¸ ëª¨ë¸ ë³´ë‹¤ ì ì–´ íš¨ìœ¨ì ì¸ layout + text ê´€ê³„ë¥¼ ì¸ì½”ë”©í•œ ê²ƒìœ¼ë¡œ <br>
**details :** [notion](https://long8v.notion.site/BROS-86b66bbdc6aa49c5914a3a93cd9bb4be)<br>

## 2021-12-02 Donut
Donut : Document Understanding Transformer without OCR([arxiv](https://arxiv.org/pdf/2111.15664.pdf))<br>
**problem :** ê¸°ì¡´ ì‚¬ì§„ ê¸°ë°˜ì˜ ë¬¸ì„œë¥¼ ì´í•´í•˜ëŠ” íƒœìŠ¤í¬ë“¤ì˜ ì ‘ê·¼ ë°©ì‹ë“¤ì€ OCRì„ í•œë²ˆ ê±°ì¹¨ìœ¼ë¡œì„œ ê³„ì‚° ë¹„ìš©ì´ í¬ê³  OCR ì—ëŸ¬ë¡œ ì¸í•´ ìƒê²¨ë‚˜ëŠ” ì„±ëŠ¥ ì €í•˜ë¬¸ì œê°€ ìˆìŒ.<br>
**solution :** Swin Transformer ì¸ì½”ë” + BART ë””ì½”ë”ë¡œ ì´ë¯¸ì§€ë¥¼ ì½ê³  ë°”ë¡œ jsonìœ¼ë¡œ í¬ë§¤íŒ… ê°€ëŠ¥í•œ í† í°ì„ ë‚´ë±‰ë„ë¡ í•¨. ì´ë•Œ í”„ë¦¬íŠ¸ë ˆì´ë‹ì€ ê°€ìƒì˜ ë¬¸ì„œ ë°ì´í„°ë¥¼ ë§Œë“¤ê³  ë¬¸ì„œ ë‚´ ê¸€ìë¥¼ ëª¨ë‘ ì½ëŠ” ê²ƒì„ í†µí•´ ì§„í–‰í•¨.<br>
**result :** ë¬¸ì„œë¶„ë¥˜ : OCR ì—†ëŠ” ëª¨ë¸ ì¤‘ì—ì„œ SOTA, OCR ê¸°ë°˜ì˜ í”„ë¦¬íŠ¸ë ˆì´ë‹ ëª¨ë¸ì¸ LayoutLMì˜ ì„±ëŠ¥ì— ê·¼ì ‘í•˜ì§€ë§Œ ëª¨ë¸ í¬ê¸°ì™€ ì†ë„ ë©´ì—ì„œ ìš°ìœ„. íŒŒì‹±ì—ì„œ SOTA.  <br>
**details :** [notion](https://long8v.notion.site/Donut-fbaf0e9e19624ae492108f1249a47aa1)<br>

## 2021-12-01 
Empirical Analysis of Unlabeled Entity Problem in Named Entity Recognition([arxiv](https://openreview.net/pdf?id=5jRVa89sZk))<br>
**problem :** NER ë¬¸ì œì—ì„œ ì‹¤ìˆ˜ í˜¹ì€ ë³µì¡ì„± ë•Œë¬¸ì— unlabeldëœ entityë“¤ì€ positive sampleì„ ì¤„ì—¬ ì„±ëŠ¥ì„ ì•…íšŒì‹œí‚¤ê¸°ë„ í•˜ì§€ë§Œ ì´ëŠ” BERTì™€ ê°™ì€ PLMìœ¼ë¡œ í•´ê²° ê°€ëŠ¥í•œ ë°˜ë©´ì—, negative sampleë¡œ ì‚¬ìš©ë¨ì— ë”°ë¼ ë°œìƒí•˜ëŠ” ì„±ëŠ¥ ì•…í™”ëŠ” í•´ê²°í•˜ê¸° ì–´ë ¤ì›€<br>
**solution :** labelëœ spanì— ëŒ€í•œ cross entropy loss + unlabelëœ spanì— ëŒ€í•´ ëœë¤ìœ¼ë¡œ ìƒ˜í”Œë§í•˜ì—¬ cross-entropy lossë¥¼ êµ¬í•¨<br>
**result :** ê°€ìƒì˜ ë°ì´í„°(ì¼ë¶€ëŸ¬ labled entity ì¼ë¶€ë¥¼ ë¹¼ë¨¹ìŒ)ì—ì„œëŠ” í•´ë‹¹ ë¬¸ì œë¥¼ ë‹¤ í•´ê²°, ì˜ annotateëœ ë°ì´í„°ì—ì„œëŠ” SOTAì— ê±°ì˜ ê·¼ì ‘, real-world dataëŠ” SOTA<br>

## 2021-11-30 Copying Network
Incorporating Copying Mechanism in Sequence-to-Sequence Learning([arxiv](https://arxiv.org/pdf/1603.06393.pdf))<br>
**problem :** Seq2Seqì—ì„œ sourceì— ìˆëŠ” í† í°ì„ì—ë„ ì‚¬ì „ì— ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ë‹¨ì–´ë¼ë©´ OOV ë¬¸ì œë¡œ ì˜ˆì¸¡í•  ìˆ˜ê°€ ì—†ìŒ<br>
**solution :** ì¼ë°˜ì ìœ¼ë¡œ Seq2Seqì—ì„œ decoderê°€ ë‹¤ìŒ í† í°ì„ generateí•˜ëŠ” modeì™¸ì— sourceì—ì„œ í† í°ì„ ê°€ì ¸ì˜¤ëŠ” copy modeë¥¼ ì •ì˜í•˜ê³ , ê° modeì—ì„œ ë‚˜ì˜¨ í™•ë¥ ê°’ì„ í•©í•˜ì—¬ NLLìœ¼ë¡œ êµ¬í•¨<br>
**result :** ìš”ì•½ íƒœìŠ¤í¬ì—ì„œ ê¸°ì¡´ RNN Seq2Seqë³´ë‹¤ ì„±ëŠ¥ ìš°ìœ„<br>
**details :** [notion](https://long8v.notion.site/CopyNet-64e60ff497cb46eb9f1e99e0c6bddaa9)<br>

## 2021-11-29 SPADE
Cost-effective End-to-end Information Extraction for Semi-structured Document Images([arxiv](https://arxiv.org/pdf/2104.08041.pdf))<br>
**problem :** Information Extractionì„ í•˜ê¸° ìœ„í•œ ê¸°ì¡´ ì„œë¸ŒíƒœìŠ¤í¬ë¥¼ ì—°ê²°í•œ pipelineì˜ ë°©ë²•ë¡ ì˜ ê²½ìš° ìœ ì§€ë³´ìˆ˜ê°€ ë§ì´ ë“¤ê³ , ëª¨ë¸ í•™ìŠµì‹œì—ë„ ê° ì„œë¸Œ íƒœìŠ¤í¬ë“¤ì— ëŒ€í•œ í† í°ë³„ annotationì´ í•„ìš”í•˜ì—¬ ë¹„ìš©ì´ ë§ì´ ë“¦<br>
**solution :** 2D Transformer êµ¬ì¡° + decoderì— copying mechanismì„ ë¶™ì—¬ì„œ tree êµ¬ì¡°(abstract syntax trees)ë¥¼ generateí•˜ëŠ” end2end ëª¨ë¸ <br>
**result :** ê¸°ì¡´ pipeline ë°©ë²•ë¡ ë³´ë‹¤ ë” ì ì€ ë¹„ìš©, ë™ì¼í•œ ì–‘ì˜ ë°ì´í„°ë¡œ ë” ì¢‹ì€ ê²°ê³¼<br>
**details :** [notion](https://long8v.notion.site/WYVERN-07583648be9c4620a7c13924b8ed7f4a)

## 2021-11-26 SQLova
SQLova: A Comprehensive Exploration on WikiSQL with Table-Aware Word Contextualization([arxiv](https://arxiv.org/pdf/1902.01069.pdf))<br>
**problem :** ìì—°ì–´ë¥¼ SQLë¡œ ë³€í™˜í•˜ëŠ” task(NL2SQL)ì—ì„œì˜ BERT ì ìš©. ì´ë•Œ, í†µìƒì  seq2seqë¥¼ í†µí•œ ì–¸ì–´ìƒì„±ì€ syntaxê°€ ì—†ì–´ NL2SQL ë¬¸ì œì— ì í•©í•˜ì§€ ì•ŠìŒ.<br>
**solution :** ìì—°ì–´ ì§ˆë¬¸ê³¼ í…Œì´ë¸”ì˜ ì»¬ëŸ¼ë“¤ì„ `[SEP]`í† í°ìœ¼ë¡œ concatí•˜ì—¬ BERTì— ë„£ìŒ. ë§ˆì§€ë§‰ BERTì˜ ë‘ê°œ layerì— biLSTM ì ìš©í•˜ì—¬ ì¿¼ë¦¬ì— ë“¤ì–´ê°€ëŠ” ìš”ì†Œë“¤(selectë¬¸ì— ë“¤ì–´ê°€ëŠ” ì»¬ëŸ¼ ë“±)ì„ ì˜ˆì¸¡í•˜ëŠ” 6ê°€ì§€ì˜ ëª¨ë“ˆë“¤ì— ëŒ€í•œ ì—°ì‚° ì§„í–‰. <br>
**result :** SOTA, í¬ë¼ìš°ë“œ ì†Œì‹± ë¶„ì„ ê²°ê³¼ human performacneë³´ë‹¤ ìš°ìœ„.<br>
**details :** [notion](https://long8v.notion.site/SQLova-6e14c9fecc5a420b9394288b14a463f4)

## 2021-11-25 SPADE
SPADE: Spatial Dependency Parsing for Semi-Structured Document Information Extraction([arxiv](https://arxiv.org/pdf/2005.00642.pdf))<br>
**problem :** Information Extraction ë¬¸ì œë¥¼ í’€ ë•Œì— í•œ ë¬¸ì¥ìœ¼ë¡œ í¼ì¹œ ë’¤(serialize), ì—¬ê¸°ì„œ NER ë¬¸ì œë¥¼ í’€ì–´ì™”ìœ¼ë‚˜ í•´ë‹¹ ë°©ë²•ë¡ ìœ¼ë¡œëŠ” ë³µì¡í•œ ê³µê°„ì  ê´€ê³„, ë¬¸ì„œì˜ êµ¬ì¡°ì  ì •ë³´ë¥¼ ë‹¤ë£° ìˆ˜ ì—†ë‹¤ëŠ” ë¬¸ì œê°€ ìˆìŒ <br>
**solution :** serialize ë‹¨ê³„ ì—†ì´ ê° í† í°ë“¤ê³¼ fieldë¥¼ ë…¸ë“œë¡œ ë‘ê³  ê·¸ ê´€ê³„ë¥¼ ì—£ì§€ë¡œ í•˜ëŠ” ê·¸ë˜í”„(=ê´€ê³„ê°€ ìˆìœ¼ë©´ 1, ì—†ìœ¼ë©´ 0ì¸ binary matrixë¡œ í‘œí˜„ë¨)ì¸ ë¥¼ ë§Œë“œëŠ” ê²ƒì„ ëª©í‘œë¡œ í•¨. ê° ë…¸ë“œë“¤ì€ attention ì—°ì‚°ì„ í†µí•´ì„œ encoding ë˜ê³  ì—£ì§€ë“¤ì€ ì¸ì½”ë”©ëœ ë²¡í„°ë“¤ì˜ ë‚´ì ì„ í†µí•´ í™•ë¥  ê°’ì„ ê°€ì§.<br>
**result :** BERT base NERê³¼ ë¹„ìŠ·í•˜ê±°ë‚˜ ë‚˜ì€ ì„±ëŠ¥<br>
**details :** [notion](https://long8v.notion.site/SPADE-6018bae80a514fc5b75a962fc69e39fd)

## 2021-09-05 TSDAE
TSDAE: Using Transformer-based Sequential Denoising Auto-Encoder for Unsupevised Sentence Embedding Learning([arxiv](https://arxiv.org/abs/2104.06979))<br>
**problem :** STS ë°ì´í„°ì…‹ì„ ì–»ëŠ” ê²ƒì€ ì–´ë ¤ìš°ë©°, finetuning ì‹œì— í…ìŠ¤íŠ¸ ë„ë©”ì¸ì˜ ì°¨ì´ ë•Œë¬¸ì— STSì˜ ì„±ëŠ¥ì´ finetuningì˜ ì„±ëŠ¥ì— ë°˜ë“œì‹œ ë¹„ë¡€í•˜ëŠ” ê²ƒì€ ì•„ë‹˜<br>
**solution :** auto-encoderì²˜ëŸ¼ encoderê°€ ë…¸ì´ì¦ˆê°€ ìˆëŠ” í…ìŠ¤íŠ¸ë¥¼ ë°›ì•„ ì €ì°¨ì›ì˜ ë²¡í„°ë¡œ í‘œí˜„í•˜ë©´ ì´ë¥¼ decoderê°€ noiseê°€ ì—†ëŠ” í…ìŠ¤íŠ¸ë¡œ ë³€í™”í•˜ëŠ” êµ¬ì¡°ë¥¼ ê°€ì§<br>k ì¸µì˜ self-attentionì„ í•˜ê³  Queryì™€ ValueëŠ” ë¬¸ì¥ì„ë² ë”©, KeyëŠ” ì´ì „ (k-1)ì¸µì˜ tì‹œì  ë””ì½”ë” íˆë“ ìŠ¤í…ì„ ì‚¬ìš©í•¨ (BARTì™€ ë‹¤ë¥´ê²Œ ì¸ì½”ë”ì˜ ëª¨ë“  time-stepì˜ íˆë“ ë²¡í„°ê°€ ì°¸ì¡° ë˜ì§€ ì•ŠìŒ) <br>
**result :** unsupervised sentence embedding ë°©ë²•ë¡ ì¸ MLM, GloVe, Sent2Vecê³¼ ë¹„êµí–ˆì„ ë•Œ ì„±ëŠ¥ìƒ ìš°ìœ„

## 2021-09-04 Faster R-CNN
Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Network([arxiv](https://arxiv.org/abs/1506.01497))<br>
**problem :** Objective Detectionì—ì„œ ì‚¬ë¬¼ì˜ ìœ„ì¹˜ë¥¼ ì°¾ëŠ” Region Proposalë‹¨ê³„ì—ì„œ selective searchê°€ ë„ˆë¬´ ì˜¤ë˜ê±¸ë¦¼<br>
**solution :** CNN + anchorìœ¼ë¡œ Region Porposalë¥¼ í•œ ë’¤ ì´í›„ ROI poolingê³¼ classifierì„ ìˆœì°¨ì ìœ¼ë¡œ í•™ìŠµì‹œí‚´. ì´ë•Œ feature mapì€ ê³µìœ í•˜ë©° ë‘˜ì˜ lossëŠ” í•©í•˜ì—¬ multi-task learning ë¨<br>
**result :** fast RCNNë³´ë‹¤ ì„±ëŠ¥ì´ ê°œì„ ë¬ìœ¼ë©° ì¶”ë¡ ì†ë„ë„ 2ë°° ì´ìƒ ë¹¨ë¼ì§<br>

## 2021-09-03 ViT
An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale ([arxiv](https://arxiv.org/abs/2010.11929))<br>
**problem :** Transformer êµ¬ì¡°ë¥¼ image ë„ë©”ì¸ì— ì ìš©<br>
**solution :** ì´ë¯¸ì§€ë¥¼ 16 by 16 blockìœ¼ë¡œ ë‚˜ëˆˆ ë’¤ ì­‰ í•€ ì„ë² ë”©ì— positional embeddingì„ ë”í•œ ë’¤ íŠ¸ëœìŠ¤í¬ë¨¸ ì¸ì½”ë” + FCNìœ¼ë¡œ êµ¬ì„±. ì´í›„ `[CLS]`í† í°ìœ¼ë¡œ ì´ë¯¸ì§€ ë¶„ë¥˜ íƒœìŠ¤í¬<br>
**result :** ë¶„ë¥˜ ë¬¸ì œì—ì„œ SOTA. ê°€ì¥ ì•„ë˜ ë ˆì´ì–´ì—ì„œë„ ê¸€ë¡œë²Œí•œ ì •ë³´ë¥¼ ì‚¬ìš©í•˜ê³  ìˆìŒì„ ì•Œ ìˆ˜ ìˆì—ˆìŒ. CNNë³´ë‹¤ localityë¼ëŠ” inductive biasê°€ ì ìŒ<br>
**further work :** NLPì²˜ëŸ¼ semi-supervised pretrainingì€ í•˜ì§€ ëª»í•¨

## 2021-09-01 LayoutLM
LayoutLM: Pre-training of Text and Layout for Document Image Understanding([arxiv](https://arxiv.org/pdf/1912.13318.pdf))<br>
**problem :** ì´ì „ê¹Œì§€ ë¬¸ì„œ êµ¬ì¡°ë¥¼ í•™ìŠµí•˜ê¸° ìœ„í•´ í…ìŠ¤íŠ¸/ì´ë¯¸ì§€ ì •ë³´ë§Œì„ í™œìš©í•˜ì˜€ìŒ<br>
**solution :** BERT ì•„í‚¤í…ì³ë¥¼ í™œìš©í•˜ì—¬, ì¢Œí‘œ, í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©í•˜ì—¬ MLM, MDC(ë¬¸ì„œë¶„ë¥˜)ë¡œ í”„ë¦¬íŠ¸ë ˆì´ë‹í•˜ê³  ë°”ìš´ë”©ë°•ìŠ¤ë‚´ ì´ë¯¸ì§€ ì •ë³´ë¥¼ Faster RCNNì—ì„œ featureë¥¼ ë½‘ì•„ ê²°í•©í•˜ì—¬ finetuning <br>
**result :** information extraction, document classification ë“±ì˜ íƒœìŠ¤í¬ì—ì„œ SOTA<br>

## 2021-08-30 BART
BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension([arxiv](https://arxiv.org/abs/1910.13461))<br>
**problem :** BERTëŠ” íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ì¸ì½”ë”ë§Œ ì‚¬ìš©, GPTëŠ” ë””ì½”ë”ë§Œ ì‚¬ìš©. ë‘ ëª¨ë¸ì˜ ì¼ë°˜í™”ëœ ëª¨ë¸ì„ ë§Œë“¤ê³  ì‹¶ìŒ <br>
**solution :** íŠ¸ëœìŠ¤í¬ë¨¸ì˜ encoder-decoder êµ¬ì¡°ë¥¼ ëª¨ë‘ ì‚¬ìš©í•˜ì—¬ noiseê°€ ë“¤ì–´ê°„ í…ìŠ¤íŠ¸ë¥¼ ì›ë˜ í…ìŠ¤íŠ¸ë¡œ ì›ë³µí•˜ë©´ì„œ pre-trainingë¨. fine-tuningì‹œì—ëŠ” inputê³¼ ouputì— sdame inputì„ ë„£ì€ ë’¤ decoderì— ë§ˆì§€ë§‰ ouputì— fcnì„ ë„£ì–´ì„œ ì‚¬ìš©í•¨<br>
**result :** MNLIë¥¼ ì œì™¸í•˜ê³  ëª¨ë“  GLUE  taskì—ì„œ BERTë³´ë‹¤ ìš°ìœ„

## 2021-08-20 
Vocabulary Learning via Optimal Transport for Neural Machine Translation([arxiv](https://arxiv.org/pdf/2012.15671.pdf))<br>
**problem :** vocabularyë¥¼ ì–´ë–»ê²Œ ì„¤ì •í•˜ëŠ”ì§€ì— ëŒ€í•´ ë¹„êµí•˜ëŠ” ê²ƒì— ëŒ€í•œ ë¹„ìš©ì´ ë„ˆë¬´ í¬ë©°, vocabì— ë”°ë¼ fine-tune ì„±ëŠ¥ì´ ë‹¬ë¼ì§ <br>
í˜„ì¬ vocabì„ ì„ ì •í•˜ëŠ” ê¸°ì¤€ì€ frequencyë‚˜ entropyê¸°ì¤€ìœ¼ë¡œ íœ´ë¦¬ìŠ¤í‹±í•˜ê²Œ ì„¤ì •ë¨<br>
**solution :** ê²½ì œí•™ì˜ Marginal Utilityì˜ ê°œë…ì„ ë¹Œë ¤, Marginal Utility of Vocabularyë¥¼ ì •ì˜í•˜ì—¬ ì´ë¥¼ ìµœëŒ€í™”í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•¨. MUVì˜ ë¯¸ë¶„ê°’ì€ vocabìˆ˜ë¥¼ ëŠ˜ë ¸ì„ ë•Œ marginalí•˜ê²Œ ëŠ˜ë ¸ì„ ë•Œ entropyì˜ ì°¨ì´ë¡œ ë‘ì–´ ì¼ì • spanì—ì„œ ì¤„ì–´ë“  ì—”íŠ¸ë¡œí”¼ê°€ ê°€ì¥ ì‘ì€ ê°’ì—ì„œ MUVê°€ ìµœëŒ€í™”ë¨.<br>
**result :** 1) ê¸°ì¡´ì— ì‚¬ìš©ë˜ë˜ vocabë³´ë‹¤ MT ì„±ëŠ¥ì´ ì¢‹ì•˜ìŒ 2) íœ´ë¦¬ìŠ¤í‹±í•˜ê²Œ(ì‹¤í—˜) ì„ ì •ëœ vocabê³¼ ì„±ëŠ¥ì´ ìœ ì‚¬í•˜ì˜€ìŒ 3) ë‹¤êµ­ì–´ vocabì—ì„œë„ ë” ì¢‹ì•˜ìŒ<br>
